---
title: Machine Learning Workflow (Simplified)
jupyter: python3
format:
  html:
    toc: true
---

## Dataset Description
---

**Note**

- This dataset originally comes from [Uber Fares Dataset](https://www.kaggle.com/datasets/yasserh/uber-fares-dataset)
- We perform several edit for this mentoring purposes. So, please use the dataset from [here](https://drive.google.com/file/d/1-Fr3OMbI1yKU_jNy-6cgXFJDVzjph3sn/view?usp=sharing).

**Description**
- We're looking to predict the fare of Uber's transactions.
- The dataset contains of the following fields

<center>

|Feature|Type|Descriptions|
|:--|:--|:--|
|`order_id`| `int` | a unique identifier for each trip|
|`pickup_time` | `str` | a class of pickup time. `04-10`, `10-16`, `16-22`, `22-04`. E.g. `04-10` means the pickup time is between 04.00 to 10.00|
| `pickup_longitude` | `float` | the longitude where the meter was engaged|
| `pickup_latitude` | `float` | the latitude where the meter was engaged|
| `dropoff_longitude` | `float` | the longitude where the meter was disengaged|
| `dropoff_latitude` | `float` | the latitude where the meter was disengaged|
| `passenger_count` | `float` | the number of passengers in the vehicle (driver entered value)|
| `fare_amount` | `int` | the cost of each trip in USD, (**our target**)|

## Modeling Workflow
---

```
1. Import data to Python
2. Data Preprocessing
3. Training a Machine Learning Models
4. Test Prediction
```

### 1. Import data to Python 
---

```{python}
# Import Numpy and Pandas library
import pandas as pd
import numpy as np
```

```{python}
# Create a function to read the data
def read_data(fname):
    data = pd.read_csv(fname)
    print('Data shape raw               :', data.shape)
    print('Number of duplicate order id :', data.duplicated(subset='order_id').sum())
    data = data.drop_duplicates(subset='order_id', keep='last')
    data = data.set_index('order_id')
    print('Data shape after dropping    :', data.shape)
    print('Data shape final             :', data.shape)
    return data
```

```{python}
# Read the Uber data
data = read_data(fname='uber_edit.csv')
```

```{python}
data.head()
```

### 2. Data Preprocessing 
---

**The processing pipeline**
```
2.1 Input-Output Split
2.2 Train-Valid-Test Split
2.3 Separate Numerical and Categorical Features
2.4 Numerical Imputation
2.5 Categorical Imputation
2.6 Preprocess Categorical Features
2.7 Join the Data
2.8 Feature Engineering the Data
2.9 Create a Preprocessing Function
```

#### 2.1. Input-Output Split 
---

- We're going to split input & output according to the modeling objective.
- Create a function to split the input & output

```{python}
def split_input_output(data, target_col):
    X = data.drop(columns=target_col)
    y = data[target_col]
    print('X shape:', X.shape)
    print('y shape:', y.shape)
    return X, y
```

```{python}
X, y = split_input_output(data=data,
                          target_col='fare_amount')
```

```{python}
X.head()  
```

```{python}
y.head()
```

#### 2.2. Train-Valid-Test Split 
---

- Now, we want to split the data before modeling.
- Split the data into three set:
  - Train, for training the model
  - Validation, for choosing the best model
  - Test, for error generalization

```{python}
from sklearn.model_selection import train_test_split
def split_train_test(X, y, test_size, seed):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)
    print('X train shape:', X_train.shape)
    print('y train shape:', y_train.shape)
    print('X test shape :', X_test.shape)
    print('y test shape :', y_test.shape)
    return X_train, X_test, y_train, y_test
```

```{python}
# Split the data
# First, split the train & not train
X_train, X_not_train, y_train, y_not_train =  split_train_test(X, y, test_size=0.2, seed=123) 

# Then, split the valid & test
X_valid, X_test, y_valid, y_test = split_train_test(X_not_train, y_not_train, test_size=0.5, seed=123)
```

```{python}
print(len(X_train)/len(X))  # should be 0.8
print(len(X_valid)/len(X))  # should be 0.1
print(len(X_test)/len(X))   # should be 0.1
```

```{python}
X_train.head()
```

#### 2.3. Separate Numerical and Categorical Features
---

- We now prepare to perform data preprocessing
- But, we first separate the data into numerical data & categorical data.

```{python}
def split_num_cat(data, num_cols, cat_cols):
    data_num = data[num_cols]
    data_cat = data[cat_cols]
    print('Data num shape:', data_num.shape)
    print('Data cat shape:', data_cat.shape)
    return data_num, data_cat
```

```{python}
num_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude' ,'passenger_count']
cat_cols = ['pickup_time']
X_train_num, X_train_cat = split_num_cat(X_train, num_cols, cat_cols) # WRITE YOUR CODE HERE
```

```{python}
X_train_num.head()
```

```{python}
X_train_cat.head()
```

#### EDA before Preprocessing
---

- Find the number of missing values

```{python}
100 * (X_train.isna().sum(0) / len(X_train))
```

- We will impute all these variables if there is any missing value

- First, check the numerical features distribution

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))
axes = ax.flatten()

for i, col in enumerate(X_train_num.columns):
    sns.kdeplot(X_train_num[col], ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')

plt.tight_layout()
plt.show()
```

- All the distribution are skewed, we can impute a missing value by its features median.

- Next, explore the `pickup_time`


```{python}
X_train['pickup_time'].value_counts(normalize=True)
```

- There's a missing value with symbol `'-'` in `pickup_time`,
- We can impute the missing value with `UNKNOWN`

- Explore the relation between `pickup_time` and `fare`

```{python}
train_data = pd.concat((X_train, y_train), axis=1)
train_data.head()
```

```{python}
sns.boxplot(data=train_data[train_data['fare_amount'] < 50],
            x='pickup_time',
            y='fare_amount')
plt.show()
```

- There is no significant fare different between `pickup_time`.
- We can perform a one hot encoding for this data.

**Conclusion for preprocessing**
- Impute the missing `passenger_counts` with its median
- Impute the missing `pickup_time` with `'UNKNOWN'`
- Feature engineering the `dropoff` and `pickup` coordinate to be a distance between pickup and dropoff. We can use an Euclidean distance for simplicity.

#### 2.4. Numerical Imputation (6 pts)
---

- Now, let's perform a numerical imputation
- First check the missing value of the numerical data

```{python}
X_train_num.isna().sum(0)
```

- Create a function to fit a numerical features imputer

```{python}
from sklearn.impute import SimpleImputer
def num_imputer_fit(data):
    imputer = SimpleImputer(strategy='median')
    imputer.fit(data)
    return imputer

def num_imputer_transform(data, imputer):
    data_imputed = imputer.transform(data)
    data_imputed = pd.DataFrame(data_imputed, columns=data.columns, index=data.index)
    return data_imputed
```

- Perform imputation

```{python}
# Get the numerical imputer
num_imputer = num_imputer_fit(X_train_num) 
# Transform the data
X_train_num_imputed = num_imputer_transform(X_train_num, num_imputer) 
```

```{python}

X_train_num_imputed.isna().sum(0)
```


#### 2.5. Categorical Imputation
---

- Next, let's perform the categorical imputation

```{python}
X_train_cat.value_counts(normalize=True)
```

- Create a function to fit a categorical features imputer

```{python}
def cat_imputer_fit(data): 
    imputer = SimpleImputer(missing_values='-', strategy='constant', fill_value='UNKNOWN')
    imputer.fit(data)
    return imputer
def cat_imputer_transform(data, imputer):
    data_imputed = imputer.transform(data)
    data_imputed = pd.DataFrame(data_imputed, columns=data.columns, index=data.index)
    return data_imputed
```

- Perform imputation

```{python}
# Perform categorical imputation
cat_imputer = cat_imputer_fit(X_train_cat) 

# Transform
X_train_cat_imputed = cat_imputer_transform(X_train_cat, cat_imputer) 
```

```{python}
X_train_cat_imputed.value_counts(normalize=True)
```

Great!

#### 2.6. Preprocess Categorical Features
---

- We will create a one-hot-encoder (read the `EDA before processing`) for the categorical features
- Create a function to perform a one hot encoder

```{python}
from sklearn.preprocessing import OneHotEncoder
def cat_encoder_fit(data):
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoder.fit(data)
    return encoder

def cat_encoder_transform(data, encoder):
    data_encoded = encoder.transform(data)
    data_encoded = pd.DataFrame(data_encoded, columns=encoder.get_feature_names_out(data.columns), index=data.index)
    return data_encoded
```

- Perform imputation

```{python}
# Perform categorical imputation
cat_encoder = cat_encoder_fit(X_train_cat_imputed) 

# Transform
X_train_cat_encoded = cat_encoder_transform(X_train_cat_imputed, cat_encoder) 
```

```{python}
print('Original shape:', X_train_cat_imputed.shape)
print('Encoded shape :', X_train_cat_encoded.shape)
```

```{python}
X_train_cat_encoded.head()
```

```{python}
X_train_cat_imputed.head()
```

Great!

#### 2.7. Join the data
---

- After all the data is filled (numerically), we can join the data
- Create a function to join the data

```{python}
def concat_data(num_data, cat_data):
    data = pd.concat((num_data, cat_data), axis=1)
    return data
```

- Perform concatenated

```{python}
X_train_concat = concat_data(X_train_num_imputed, X_train_cat_encoded)
print('Numerical data shape  :', X_train_num_imputed.shape)
print('Categorical data shape:', X_train_cat_encoded.shape)
print('Concat data shape     :', X_train_concat.shape)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Validate
X_train_concat.head()
```


#### 2.8. Feature engineering the data
---

- Now, `pickup` and `dropoff` coordinate is not an explicit features.
- We can create a better feature called by `distance` to summarize the `pickup` and `dropoff` coordinate.

```{python}
def map_distance(data):
    data['distance'] = np.sqrt((data['pickup_longitude'] - data['dropoff_longitude'])**2 + (data['pickup_latitude'] - data['dropoff_latitude'])**2)
    data = data.drop(columns=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'])
    return data
```

- Perform distance calculation

```{python}
X_train_concat_fe = map_distance(X_train_concat)
print('Original data shape:', X_train_concat.shape)
print('Mapped data shape  :', X_train_concat_fe.shape)
```

```{python}
X_train_concat_fe.head()
```

- And finally, we standardize the data so that it can perform well during model optimization

```{python}
from sklearn.preprocessing import StandardScaler
def fit_scaler(data):
    scaler = StandardScaler()
    scaler.fit(data)
    return scaler
def transform_scaler(data, scaler):
    data_scaled = scaler.transform(data)
    data_scaled = pd.DataFrame(data_scaled, columns=data.columns, index=data.index)
    return data_scaled
```

```{python}
# Fit the scaler
scaler = fit_scaler(X_train_concat_fe) 
# Transform the scaler
X_train_clean = transform_scaler(X_train_concat_fe, scaler)
```

```{python}
X_train_clean.describe().round(4)
```


#### 2.9. Create the preprocess function
---

- Now, let's create a function to preprocess other set of data (valid & test) so that we can predict that

```{python}
def preprocess_data(data, num_cols, cat_cols, num_imputer, cat_imputer, cat_encoder, scaler):
    data_num, data_cat = split_num_cat(data, num_cols, cat_cols)
    data_num_imputed = num_imputer_transform(data_num, num_imputer)
    data_cat_imputed = cat_imputer_transform(data_cat, cat_imputer)
    data_cat_encoded = cat_encoder_transform(data_cat_imputed, cat_encoder)
    data_concat = concat_data(data_num_imputed, data_cat_encoded)
    data_mapped = map_distance(data_concat)
    data_scaled = transform_scaler(data_mapped, scaler)
    return data_scaled

```

```{python}
X_train_clean = preprocess_data(data=X_train, 
                                num_cols=num_cols, 
                                cat_cols=cat_cols, 
                                num_imputer=num_imputer, 
                                cat_imputer=cat_imputer, 
                                cat_encoder=cat_encoder, 
                                scaler=scaler)

print('Numerical data shape  :', X_train_num_imputed.shape)
print('Categorical data shape:', X_train_cat_encoded.shape)
print('Concat data shape     :', X_train_concat.shape)
print('Original data shape:', X_train_concat.shape)
print('Mapped data shape  :', X_train_concat_fe.shape)
```

```{python}
print('Original data shape:', X_train.shape)
print('Cleaned data shape :', X_train_clean.shape)
X_train_clean.head()
```

```{python}
X_valid_clean = preprocess_data(data=X_valid, 
                                num_cols=num_cols, 
                                cat_cols=cat_cols, 
                                num_imputer=num_imputer, 
                                cat_imputer=cat_imputer, 
                                cat_encoder=cat_encoder, 
                                scaler=scaler)
X_test_clean = preprocess_data(data=X_test, 
                               num_cols=num_cols, 
                               cat_cols=cat_cols, 
                               num_imputer=num_imputer, 
                               cat_imputer=cat_imputer, 
                               cat_encoder=cat_encoder, 
                               scaler=scaler)
print('Cleaned X_valid data shape :', X_valid_clean.shape)
print('Cleaned X_test data shape :', X_test_clean.shape)
```

### 3. Training Machine Learning Models
---

```
3.1 Prepare train & evaluate model function
3.2 Train & evaluate several models
3.3 Choose the best model
```

#### 3.1. Preprare train & evaluate model function
---

- Before modeling, let's prepare function to train & evaluate model

```{python}
from sklearn.metrics import mean_squared_error
def train_model(estimator, X_train, y_train):
    estimator.fit(X_train, y_train)
    
def evaluate_model(estimator, X_train, y_train, X_valid, y_valid):
    y_train_pred = estimator.predict(X_train)
    y_valid_pred = estimator.predict(X_valid)
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    rmse_valid = np.sqrt(mean_squared_error(y_valid, y_valid_pred))
    return rmse_train, rmse_valid
```

#### 3.2. Train and Evaluate Several Models
---

- Now, let's train & evaluate several models
- You should check, which one of the following model is the best model

  1. Baseline model
  2. k-NN with k=1
  3. k-NN with k=100
  4. k-NN with k=200

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.dummy import DummyRegressor
reg_1 = DummyRegressor()
reg_2 = KNeighborsRegressor(n_neighbors=1)
reg_3 = KNeighborsRegressor(n_neighbors=100) 
reg_4 = KNeighborsRegressor(n_neighbors=200)
```

```{python}
# Train the model
train_model(reg_1, X_train_clean, y_train)
train_model(reg_2, X_train_clean, y_train)
train_model(reg_3, X_train_clean, y_train)
train_model(reg_4, X_train_clean, y_train)
```

```{python}
import time

for reg in [reg_1, reg_2, reg_3, reg_4]:
    t0 = time.time()

    # Generate the rmse
    rmse_train, rmse_valid = evaluate_model(estimator=reg,
                                            X_train=X_train_clean,
                                            y_train=y_train,
                                            X_valid=X_valid_clean,
                                            y_valid=y_valid)

    # Logging
    elapsed = time.time() - t0
    print(f'model : {str(reg):40s} '
          f'| RMSE train: {rmse_train:.4f} '
          f'| RMSE valid: {rmse_valid:.4f} '
          f'| Time elapsed: {elapsed*1000:.2f} ms')
```


#### 3.3. Choose the best model 
---

From the previous results, which one is the best model?

Why do you choose that model?

And, create a `reg_best` to store the best model

```{python}
reg_best = reg_3
```

### 4. Predictions & Evaluations
---

```
4.1 Predict & Evaluate on the Train Data
4.2 Predict & Evaluate on the Test Data
```

#### 4.1. Predict & evaluate on train data
---

```{python}
# Predict
y_train_pred = reg_best.predict(X_train_clean)
```

```{python}
plt.scatter(y_train, y_train_pred)

plt.plot([0, 200], [0, 200], c='red')
plt.xlim(0, 200); plt.ylim(0, 200)
plt.xlabel('y actual'); plt.ylabel('y predicted')
plt.title('Comparison of y actual vs y predicted on Train Data')
plt.show()
```

#### 4.2. Predict & evaluate on test data
---

```{python}
# Predict
y_test_pred = reg_best.predict(X_test_clean)
```

```{python}
# Visualize & compare the prediction
plt.scatter(y_test, y_test_pred)

plt.plot([0, 200], [0, 200], c='red')
plt.xlim(0, 200); plt.ylim(0, 200)
plt.xlabel('y actual'); plt.ylabel('y predicted')
plt.title('Comparison of y actual vs y predicted on Test Data')
plt.show()
```

```{python}
# RMSE 
rmse_train, rmse_test = evaluate_model(estimator=reg_best,
                                            X_train=X_train_clean,
                                            y_train=y_train,
                                            X_valid=X_test_clean,
                                            y_valid=y_test)
```

```{python}
print(f'| RMSE train: {rmse_train:.4f} '
      f'| RMSE test: {rmse_test:.4f} ')
```

