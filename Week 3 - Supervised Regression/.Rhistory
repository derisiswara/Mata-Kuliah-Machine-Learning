# Import Numpy and Pandas library
import pandas as pd
import numpy as np
# Create a function to read the data
def read_data(fname):
data = pd.read_csv(fname)
print('Data shape raw               :', data.shape)
print('Number of duplicate order id :', data.duplicated(subset='order_id').sum())
data = data.drop_duplicates(subset='order_id', keep='last')
data = data.set_index('order_id')
print('Data shape after dropping    :', data.shape)
print('Data shape final             :', data.shape)
return data
# Read the Uber data (JUST RUN THE CODE)
data = read_data(fname='uber_edit.csv')
data.head()
def split_input_output(data, target_col):
X = data.drop(columns=target_col)
y = data[target_col]
print('X shape:', X.shape)
print('y shape:', y.shape)
return X, y
X, y = split_input_output(data=data,
target_col='fare_amount')
X.head()
y.head()
from sklearn.model_selection import train_test_split
def split_train_test(X, y, test_size, seed):
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)
print('X train shape:', X_train.shape)
print('y train shape:', y_train.shape)
print('X test shape :', X_test.shape)
print('y test shape :', y_test.shape)
return X_train, X_test, y_train, y_test
# Split the data
# First, split the train & not train
X_train, X_not_train, y_train, y_not_train =  split_train_test(X, y, test_size=0.2, seed=123)
# Then, split the valid & test
X_valid, X_test, y_valid, y_test = split_train_test(X_not_train, y_not_train, test_size=0.5, seed=123)
print(len(X_train)/len(X))  # should be 0.8
print(len(X_valid)/len(X))  # should be 0.1
print(len(X_test)/len(X))   # should be 0.1
X_train.head()
def split_num_cat(data, num_cols, cat_cols):
data_num = data[num_cols]
data_cat = data[cat_cols]
print('Data num shape:', data_num.shape)
print('Data cat shape:', data_cat.shape)
return data_num, data_cat
num_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude' ,'passenger_count']
cat_cols = ['pickup_time']
X_train_num, X_train_cat = split_num_cat(X_train, num_cols, cat_cols) # WRITE YOUR CODE HERE
X_train_num.head()
X_train_cat.head()
100 * (X_train.isna().sum(0) / len(X_train))
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))
axes = ax.flatten()
for i, col in enumerate(X_train_num.columns):
sns.kdeplot(X_train_num[col], ax=axes[i])
axes[i].set_title(f'Distribution of {col}')
plt.tight_layout()
plt.show()
X_train['pickup_time'].value_counts(normalize=True)
train_data = pd.concat((X_train, y_train), axis=1)
train_data.head()
sns.boxplot(data=train_data[train_data['fare_amount'] < 50],
x='pickup_time',
y='fare_amount')
plt.show()
X_train_num.isna().sum(0)
from sklearn.impute import SimpleImputer
def num_imputer_fit(data):
imputer = SimpleImputer(strategy='median')
imputer.fit(data)
return imputer
def num_imputer_transform(data, imputer):
data_imputed = imputer.transform(data)
data_imputed = pd.DataFrame(data_imputed, columns=data.columns, index=data.index)
return data_imputed
# Get the numerical imputer
num_imputer = num_imputer_fit(X_train_num)
# Transform the data
X_train_num_imputed = num_imputer_transform(X_train_num, num_imputer)
X_train_num_imputed.isna().sum(0)
X_train_cat.value_counts(normalize=True)
def cat_imputer_fit(data):
imputer = SimpleImputer(missing_values='-', strategy='constant', fill_value='UNKNOWN')
imputer.fit(data)
return imputer
def cat_imputer_transform(data, imputer):
data_imputed = imputer.transform(data)
data_imputed = pd.DataFrame(data_imputed, columns=data.columns, index=data.index)
return data_imputed
# Perform categorical imputation
cat_imputer = cat_imputer_fit(X_train_cat)
# Transform
X_train_cat_imputed = cat_imputer_transform(X_train_cat, cat_imputer)
X_train_cat_imputed.value_counts(normalize=True)
from sklearn.preprocessing import OneHotEncoder
def cat_encoder_fit(data):
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoder.fit(data)
return encoder
def cat_encoder_transform(data, encoder):
data_encoded = encoder.transform(data)
data_encoded = pd.DataFrame(data_encoded, columns=encoder.get_feature_names_out(data.columns), index=data.index)
return data_encoded
# Perform categorical imputation
cat_encoder = cat_encoder_fit(X_train_cat_imputed)
# Transform
X_train_cat_encoded = cat_encoder_transform(X_train_cat_imputed, cat_encoder)
print('Original shape:', X_train_cat_imputed.shape)
print('Encoded shape :', X_train_cat_encoded.shape)
X_train_cat_encoded.head()
X_train_cat_imputed.head()
def concat_data(num_data, cat_data):
data = pd.concat((num_data, cat_data), axis=1)
return data
X_train_concat = concat_data(X_train_num_imputed, X_train_cat_encoded)
print('Numerical data shape  :', X_train_num_imputed.shape)
print('Categorical data shape:', X_train_cat_encoded.shape)
print('Concat data shape     :', X_train_concat.shape)
#| colab: {base_uri: https://localhost:8080/}
# Validate
X_train_concat.head()
def map_distance(data):
data['distance'] = np.sqrt((data['pickup_longitude'] - data['dropoff_longitude'])**2 + (data['pickup_latitude'] - data['dropoff_latitude'])**2)
data = data.drop(columns=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'])
return data
X_train_concat_fe = map_distance(X_train_concat)
print('Original data shape:', X_train_concat.shape)
print('Mapped data shape  :', X_train_concat_fe.shape)
X_train_concat_fe.head()
from sklearn.preprocessing import StandardScaler
def fit_scaler(data):
scaler = StandardScaler()
scaler.fit(data)
return scaler
def transform_scaler(data, scaler):
data_scaled = scaler.transform(data)
data_scaled = pd.DataFrame(data_scaled, columns=data.columns, index=data.index)
return data_scaled
# Fit the scaler
scaler = fit_scaler(X_train_concat_fe)
# Transform the scaler
X_train_clean = transform_scaler(X_train_concat_fe, scaler)
X_train_clean.describe().round(4)
def preprocess_data(data, num_cols, cat_cols, num_imputer, cat_imputer, cat_encoder, scaler):
data_num, data_cat = split_num_cat(data, num_cols, cat_cols)
data_num_imputed = num_imputer_transform(data_num, num_imputer)
data_cat_imputed = cat_imputer_transform(data_cat, cat_imputer)
data_cat_encoded = cat_encoder_transform(data_cat_imputed, cat_encoder)
data_concat = concat_data(data_num_imputed, data_cat_encoded)
data_mapped = map_distance(data_concat)
data_scaled = transform_scaler(data_mapped, scaler)
return data_scaled
X_train_clean = preprocess_data(data=X_train,
num_cols=num_cols,
cat_cols=cat_cols,
num_imputer=num_imputer,
cat_imputer=cat_imputer,
cat_encoder=cat_encoder,
scaler=scaler)
print('Numerical data shape  :', X_train_num_imputed.shape)
print('Categorical data shape:', X_train_cat_encoded.shape)
print('Concat data shape     :', X_train_concat.shape)
print('Original data shape:', X_train_concat.shape)
print('Mapped data shape  :', X_train_concat_fe.shape)
print('Original data shape:', X_train.shape)
print('Cleaned data shape :', X_train_clean.shape)
X_train_clean.head()
X_valid_clean = preprocess_data(data=X_valid,
num_cols=num_cols,
cat_cols=cat_cols,
num_imputer=num_imputer,
cat_imputer=cat_imputer,
cat_encoder=cat_encoder,
scaler=scaler)
X_test_clean = preprocess_data(data=X_test,
num_cols=num_cols,
cat_cols=cat_cols,
num_imputer=num_imputer,
cat_imputer=cat_imputer,
cat_encoder=cat_encoder,
scaler=scaler)
print('Cleaned X_valid data shape :', X_valid_clean.shape)
print('Cleaned X_test data shape :', X_test_clean.shape)
from sklearn.metrics import mean_squared_error
def train_model(estimator, X_train, y_train):
estimator.fit(X_train, y_train)
def evaluate_model(estimator, X_train, y_train, X_valid, y_valid):
y_train_pred = estimator.predict(X_train)
y_valid_pred = estimator.predict(X_valid)
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
rmse_valid = np.sqrt(mean_squared_error(y_valid, y_valid_pred))
return rmse_train, rmse_valid
####################################################
# Create your model here (no need to create function)
# Write your code here
####################################################
'''
- Now, let's train & evaluate several models
- You should check, which one of the following model is the best model
1. Baseline model
2. k-NN with k=1
3. k-NN with k=100
4. k-NN with k=200
'''
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.dummy import DummyRegressor
reg_1 = DummyRegressor()
reg_2 = KNeighborsRegressor(n_neighbors=1)
reg_3 = KNeighborsRegressor(n_neighbors=100)
reg_4 = KNeighborsRegressor(n_neighbors=200)
# Train the model (JUST RUN THE CODE)
train_model(reg_1, X_train_clean, y_train)
train_model(reg_2, X_train_clean, y_train)
train_model(reg_3, X_train_clean, y_train)
train_model(reg_4, X_train_clean, y_train)
import time
for reg in [reg_1, reg_2, reg_3, reg_4]:
t0 = time.time()
# Generate the rmse
rmse_train, rmse_valid = evaluate_model(estimator=reg,
X_train=X_train_clean,
y_train=y_train,
X_valid=X_valid_clean,
y_valid=y_valid)
# Logging
elapsed = time.time() - t0
print(f'model : {str(reg):40s} '
f'| RMSE train: {rmse_train:.4f} '
f'| RMSE valid: {rmse_valid:.4f} '
f'| Time elapsed: {elapsed*1000:.2f} ms')
reg_best = reg_3
# Predict
y_train_pred = reg_best.predict(X_train_clean)
plt.scatter(y_train, y_train_pred)
plt.plot([0, 200], [0, 200], c='red')
plt.xlim(0, 200); plt.ylim(0, 200)
plt.xlabel('y actual'); plt.ylabel('y predicted')
plt.title('Comparison of y actual vs y predicted on Train Data')
plt.show()
# Predict
y_test_pred = reg_best.predict(X_test_clean)
# Visualize & compare the prediction
plt.scatter(y_test, y_test_pred)
plt.plot([0, 200], [0, 200], c='red')
plt.xlim(0, 200); plt.ylim(0, 200)
plt.xlabel('y actual'); plt.ylabel('y predicted')
plt.title('Comparison of y actual vs y predicted on Test Data')
plt.show()
# RMSE
rmse_train, rmse_test = evaluate_model(estimator=reg_best,
X_train=X_train_clean,
y_train=y_train,
X_valid=X_test_clean,
y_valid=y_test)
print(f'| RMSE train: {rmse_train:.4f} '
f'| RMSE test: {rmse_test:.4f} ')
